{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c49bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdaeeb",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914a6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2a79e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in as M00nl8tshad0w\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder, whoami\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "HfFolder.save_token(hf_token)\n",
    "user = whoami()\n",
    "print(f\"logged in as {user[\"name\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9b51df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory: /home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_models/GLM-4-Z1-9B-0414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f230c4c7c6499fa1a82964a7be50e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_name, local_dir=\"./models/llama3_70b\"):\n",
    "    if os.path.exists(local_dir):\n",
    "        print(f\"Loading model from local directory: {local_dir}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "        model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "    else:\n",
    "        print(f\"Local directory not found. Downloading model '{model_name}' from Hugging Face Hub...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        tokenizer.save_pretrained(local_dir)\n",
    "        model.save_pretrained(local_dir)\n",
    "        print(f\"Model downloaded and saved locally to: {local_dir}\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "#tokenizer, model = load_model(model_name=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "#                              local_dir=\"/home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_models/llama3_70b/\")\n",
    "\n",
    "tokenizer, model = load_model(model_name=\"THUDM/GLM-4-Z1-9B-0414\",\n",
    "                              local_dir=\"/home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_models/GLM-4-Z1-9B-0414\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7b2be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7c3a40d3984f39b8842680bc5d99bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_hf_dataset(dataset_name, subset=\"default\", local_dir=\"~/hf_datasets/OpenR1_Math_220k/\"):\n",
    "    local_dir = os.path.expanduser(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    return load_dataset(dataset_name, subset, cache_dir=local_dir)\n",
    "\n",
    "math_dataset = load_hf_dataset(dataset_name=\"open-r1/OpenR1-Math-220k\",\n",
    "                               local_dir=\"/home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_datasets/open-r1/OpenR1-Math-220k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1231523",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0d4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs = tokenizer([\"What is 1+1? Give a short answer.\"], return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73427ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: GenerateDecoderOnlyOutput(sequences=tensor([[ 3838,   374,   220,    16,    10,    16,    30, 20635,   264,  2805,\n",
      "          4226,    13, 13699,   766,   397, 32169,    11,   773,   279,  1196,\n",
      "           374, 10156,    11,   330,  3838,   374,   220,    16,    10,    16,\n",
      "          7521,   323,  6801,   264,  2805,  4226,    13,  6771,   752,  1744,\n",
      "            13,  8324,    11,   220,    16,  5519,   220,    16,   374,  5020,\n",
      "         30222,    13,   758,  6770, 34612,    11,  7842,   220,    16,   323,\n",
      "           220,    16]], device='cuda:0'), scores=(tensor([[-1.8047, -2.4531, -1.5234,  ..., -1.5781, -1.5781, -1.5625]],\n",
      "       device='cuda:0'), tensor([[ 3.7031,  3.0156,  5.6875,  ..., -6.4375, -6.4375, -6.4375]],\n",
      "       device='cuda:0'), tensor([[ 5.5000, -2.2188, -1.2422,  ...,  0.9062,  0.9062,  0.9062]],\n",
      "       device='cuda:0'), tensor([[-0.3926,  2.1875,  3.3906,  ..., -2.6406, -2.6406, -2.6406]],\n",
      "       device='cuda:0'), tensor([[ 7.5625, -2.2969, -4.0625,  ..., -2.8906, -2.8906, -2.8906]],\n",
      "       device='cuda:0'), tensor([[ 0.3926, -0.8320, -2.7344,  ..., -0.0215, -0.0215, -0.0115]],\n",
      "       device='cuda:0'), tensor([[ 5.6875,  2.7656, -0.7773,  ..., -1.0469, -1.0469, -1.0391]],\n",
      "       device='cuda:0'), tensor([[ 2.3281,  0.5078, -1.7422,  ..., -1.4688, -1.4688, -1.4688]],\n",
      "       device='cuda:0'), tensor([[ 2.0000,  0.5820, -0.1836,  ..., -0.8516, -0.8516, -0.8516]],\n",
      "       device='cuda:0'), tensor([[ 1.5547, -2.7812, -2.4844,  ..., -0.9805, -0.9805, -0.9844]],\n",
      "       device='cuda:0'), tensor([[ 1.9531,  6.3750, -1.5547,  ..., -1.6328, -1.6328, -1.6250]],\n",
      "       device='cuda:0'), tensor([[-4.2500,  1.4219, -4.0938,  ..., -1.3672, -1.3672, -1.3672]],\n",
      "       device='cuda:0'), tensor([[-0.1260, -2.1406, -0.0227,  ..., -0.4961, -0.4961, -0.4980]],\n",
      "       device='cuda:0'), tensor([[ 3.8281,  5.2812, -2.3281,  ..., -2.3281, -2.3281, -2.3281]],\n",
      "       device='cuda:0'), tensor([[ 1.2578,  1.5078, -3.9219,  ..., -0.5352, -0.5352, -0.5352]],\n",
      "       device='cuda:0'), tensor([[ 2.5625,  0.8438,  0.3555,  ..., -2.4688, -2.4688, -2.4688]],\n",
      "       device='cuda:0'), tensor([[2.9531, 4.5312, 2.3281,  ..., 1.3750, 1.3750, 1.3750]],\n",
      "       device='cuda:0'), tensor([[ 4.5938, -1.9922, -0.4863,  ..., -2.6719, -2.6719, -2.6719]],\n",
      "       device='cuda:0'), tensor([[6.3750, 9.8750, 1.1953,  ..., 1.8516, 1.8516, 1.8516]],\n",
      "       device='cuda:0'), tensor([[ 4.3750, -1.8125,  2.0000,  ..., -0.7383, -0.7383, -0.7070]],\n",
      "       device='cuda:0'), tensor([[ 0.4121, -2.1719, -1.5938,  ..., -0.9492, -0.9492, -0.9492]],\n",
      "       device='cuda:0'), tensor([[ 0.8242, -0.5625, -2.5625,  ..., -1.7109, -1.7109, -1.7109]],\n",
      "       device='cuda:0'), tensor([[ 1.4375,  0.6250,  0.2422,  ..., -0.9648, -0.9648, -0.9727]],\n",
      "       device='cuda:0'), tensor([[ 3.5625,  1.0156, -1.3906,  ..., -0.7188, -0.7188, -0.7188]],\n",
      "       device='cuda:0'), tensor([[ 7.2188,  2.2656, -1.5234,  ..., -0.1147, -0.1147, -0.1016]],\n",
      "       device='cuda:0'), tensor([[-0.9102, -2.0625, -1.1094,  ...,  1.3438,  1.3438,  1.3750]],\n",
      "       device='cuda:0'), tensor([[ 2.3906,  1.2891, -0.1689,  ..., -0.1187, -0.1187, -0.1113]],\n",
      "       device='cuda:0'), tensor([[1.8047, 1.1797, 0.3223,  ..., 0.4473, 0.4473, 0.4492]],\n",
      "       device='cuda:0'), tensor([[ 7.3125,  2.8906, -0.7305,  ..., -0.4512, -0.4512, -0.4434]],\n",
      "       device='cuda:0'), tensor([[-0.8359, -4.3750, -0.5078,  ...,  1.0312,  1.0312,  1.0625]],\n",
      "       device='cuda:0'), tensor([[ 7.5625, -2.0156,  1.6641,  ..., -1.5234, -1.5234, -1.5156]],\n",
      "       device='cuda:0'), tensor([[ 2.2812, -2.7656,  0.1738,  ...,  1.8828,  1.8828,  1.8984]],\n",
      "       device='cuda:0'), tensor([[ 1.5391, -2.8906, -1.0312,  ..., -1.1719, -1.1719, -1.1641]],\n",
      "       device='cuda:0'), tensor([[3.8281, 0.5859, 0.8047,  ..., 0.2432, 0.2432, 0.2500]],\n",
      "       device='cuda:0'), tensor([[ 3.6094, -1.1328, -4.2812,  ..., -1.0859, -1.0859, -1.0781]],\n",
      "       device='cuda:0'), tensor([[ 2.3906, -1.0469,  0.6719,  ..., -2.4531, -2.4531, -2.4531]],\n",
      "       device='cuda:0'), tensor([[ 5.9375,  2.3281, -0.4004,  ..., -1.5859, -1.5859, -1.5781]],\n",
      "       device='cuda:0'), tensor([[ 2.6094, -0.9609, -0.5078,  ..., -1.4844, -1.4844, -1.4766]],\n",
      "       device='cuda:0'), tensor([[ 3.9062, -1.7578, -0.7148,  ..., -1.4141, -1.4141, -1.4141]],\n",
      "       device='cuda:0'), tensor([[ 6.3438,  0.9258, -2.8594,  ..., -0.6406, -0.6406, -0.6250]],\n",
      "       device='cuda:0'), tensor([[ 0.5312, -4.8438, -1.5391,  ...,  1.3516,  1.3516,  1.3828]],\n",
      "       device='cuda:0'), tensor([[ 3.2344, -2.2188, -0.5781,  ..., -0.2393, -0.2393, -0.2373]],\n",
      "       device='cuda:0'), tensor([[ 4.1875, -0.0195,  0.3945,  ..., -0.6523, -0.6523, -0.6484]],\n",
      "       device='cuda:0'), tensor([[ 3.6562,  2.1406, -1.2344,  ..., -1.7188, -1.7188, -1.7109]],\n",
      "       device='cuda:0'), tensor([[ 0.2139, -2.9375, -1.6719,  ..., -1.0703, -1.0703, -1.0625]],\n",
      "       device='cuda:0'), tensor([[ 0.2031, -1.6406, -1.8047,  ..., -1.6250, -1.6250, -1.6172]],\n",
      "       device='cuda:0'), tensor([[ 0.1562, -1.5469, -0.7734,  ..., -2.5156, -2.5156, -2.5156]],\n",
      "       device='cuda:0'), tensor([[ 4.0234e-01, -1.8359e+00, -7.5391e-01,  ...,  1.1292e-03,\n",
      "          1.1063e-03,  6.3477e-03]], device='cuda:0'), tensor([[ 2.1094, -3.7812, -3.6094,  ..., -1.7266, -1.7266, -1.7188]],\n",
      "       device='cuda:0'), tensor([[ 1.5000, -2.9062, -0.1631,  ..., -4.2812, -4.2812, -4.2812]],\n",
      "       device='cuda:0')), logits=None, attentions=None, hidden_states=None, past_key_values=<transformers.cache_utils.DynamicCache object at 0x77ecf85cab70>)\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Print the scores for each token generated with Greedy Search\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, return_dict_in_generate=True, output_scores=True)\n",
    "print(f\"Generated: {outputs}\")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9727bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| token | token string | log probability | probability\n",
      "| 13699 | <th      | -0.170 | 84.36%\n",
      "|   766 | ink      | 0.000 | 100.00%\n",
      "|   397 | >\n",
      "       | -0.000 | 100.00%\n",
      "| 32169 | Okay     | -0.002 | 99.76%\n",
      "|    11 | ,        | -0.000 | 100.00%\n",
      "|   773 |  so      | -0.421 | 65.65%\n",
      "|   279 |  the     | -0.226 | 79.74%\n",
      "|  1196 |  user    | -0.430 | 65.07%\n",
      "|   374 |  is      | -0.181 | 83.44%\n",
      "| 10156 |  asking  | -0.000 | 100.00%\n",
      "|    11 | ,        | -0.657 | 51.82%\n",
      "|   330 |  \"       | -0.000 | 99.99%\n",
      "|  3838 | What     | -0.000 | 100.00%\n",
      "|   374 |  is      | -0.000 | 100.00%\n",
      "|   220 |          | -0.000 | 100.00%\n",
      "|    16 | 1        | -0.000 | 100.00%\n",
      "|    10 | +        | -0.002 | 99.84%\n",
      "|    16 | 1        | -0.000 | 100.00%\n",
      "|  7521 | ?\"       | -0.100 | 90.46%\n",
      "|   323 |  and     | -0.073 | 92.95%\n",
      "|  6801 |  wants   | -0.349 | 70.56%\n",
      "|   264 |  a       | -0.000 | 100.00%\n",
      "|  2805 |  short   | -0.000 | 99.98%\n",
      "|  4226 |  answer  | -0.000 | 100.00%\n",
      "|    13 | .        | -0.001 | 99.94%\n",
      "|  6771 |  Let     | -0.019 | 98.15%\n",
      "|   752 |  me      | -0.008 | 99.24%\n",
      "|  1744 |  think   | -0.122 | 88.52%\n",
      "|    13 | .        | -0.826 | 43.78%\n",
      "|  8324 |  Well    | -0.706 | 49.37%\n",
      "|    11 | ,        | -0.000 | 100.00%\n",
      "|   220 |          | -0.256 | 77.41%\n",
      "|    16 | 1        | -0.000 | 100.00%\n",
      "|  5519 |  plus    | -0.010 | 99.03%\n",
      "|   220 |          | -0.000 | 100.00%\n",
      "|    16 | 1        | -0.000 | 100.00%\n",
      "|   374 |  is      | -0.013 | 98.75%\n",
      "|  5020 |  pretty  | -0.522 | 59.32%\n",
      "| 30222 |  straightforward | -0.637 | 52.87%\n",
      "|    13 | .        | -0.209 | 81.15%\n",
      "|   758 |  In      | -0.832 | 43.52%\n",
      "|  6770 |  basic   | -0.018 | 98.19%\n",
      "| 34612 |  arithmetic | -0.006 | 99.37%\n",
      "|    11 | ,        | -0.001 | 99.93%\n",
      "|  7842 |  adding  | -0.123 | 88.40%\n",
      "|   220 |          | -0.696 | 49.88%\n",
      "|    16 | 1        | -0.000 | 100.00%\n",
      "|   323 |  and     | -0.055 | 94.65%\n",
      "|   220 |          | -0.014 | 98.59%\n",
      "|    16 | 1        | -0.000 | 99.99%\n"
     ]
    }
   ],
   "source": [
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "print(\"| token | token string | log probability | probability\")\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | log probability | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().cpu().numpy():.3f} | {np.exp(score.detach().cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea0f41",
   "metadata": {},
   "source": [
    "### Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdf97769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(logits)=50\n",
      "logits[0].shape=torch.Size([1, 151552])\n",
      "logits[0]=tensor([[-1.8047, -2.4531, -1.5234,  ..., -1.5781, -1.5781, -1.5625]],\n",
      "       device='cuda:0')\n",
      "probabilities[0].shape=torch.Size([1, 151552])\n",
      "probabilities[0]=tensor([[3.4848e-09, 1.8220e-09, 4.6165e-09,  ..., 4.3709e-09, 4.3709e-09,\n",
      "         4.4397e-09]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.scores  # This is a list of logits for each token generated\n",
    "print(f\"{len(logits)=}\")\n",
    "print(f\"{logits[0].shape=}\")\n",
    "print(f\"{logits[0]=}\")\n",
    "probabilities = [torch.nn.functional.softmax(logit, dim=-1) for logit in logits] # Convert logits to probabilities using softmax\n",
    "print(f\"{probabilities[0].shape=}\")\n",
    "print(f\"{probabilities[0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db81dd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.4848e-09, 1.8220e-09, 4.6165e-09, 8.1658e-09, 1.4641e-09, 5.8816e-09,\n",
       "        1.3385e-08, 2.2071e-07, 1.3280e-08, 1.3330e-09], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[0].squeeze(0)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "93e86ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_distributions(probabilites: list) -> list:\n",
    "    distributions = []\n",
    "    for token in probabilites:\n",
    "        token_probabilities = {}\n",
    "        token_tensor = token.squeeze(0)\n",
    "        \"\"\"prob_values = torch.isclose(token_tensor, torch.tensor(float(0.0)))\n",
    "        for i, bool in enumerate(prob_values):\n",
    "            if not bool:\"\"\"\n",
    "        threshold = 1e-3\n",
    "        for i, prob in enumerate(token_tensor):\n",
    "            if prob.item() > threshold:\n",
    "                actual_token = tokenizer.decode(i)\n",
    "                token_prob = token_tensor[i]\n",
    "                print(f\"Probability of '{actual_token}':\", end=' ')\n",
    "                print(f\"{token_prob.item():.4%}\")\n",
    "                token_probabilities[actual_token] = token_prob\n",
    "        print(\"----\")\n",
    "        distributions.append(token_probabilities)\n",
    "    return distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66f4d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of ' ': 3.2707%\n",
      "Probability of ' (': 0.2522%\n",
      "Probability of ' A': 0.1051%\n",
      "Probability of ' -': 0.1191%\n",
      "Probability of ' The': 1.0618%\n",
      "Probability of ' \n",
      "': 0.2685%\n",
      "Probability of ' In': 0.2226%\n",
      "Probability of ' It': 0.2685%\n",
      "Probability of ' If': 0.9975%\n",
      "Probability of '  \n",
      "': 0.1350%\n",
      "Probability of ' \n",
      "\n",
      "': 3.7061%\n",
      "Probability of ' Then': 0.3447%\n",
      "Probability of ' Also': 0.9371%\n",
      "Probability of '<th': 84.3512%\n",
      "Probability of ' Explain': 0.4426%\n",
      "----\n",
      "Probability of 'ink': 100.0000%\n",
      "----\n",
      "Probability of '>\n",
      "': 99.9998%\n",
      "----\n",
      "Probability of 'Okay': 99.7585%\n",
      "Probability of 'Alright': 0.1926%\n",
      "----\n",
      "Probability of ',': 99.9958%\n",
      "----\n",
      "Probability of ' the': 24.1514%\n",
      "Probability of ' so': 65.6504%\n",
      "Probability of ' let': 10.0678%\n",
      "----\n",
      "Probability of ' the': 79.7350%\n",
      "Probability of ' I': 20.1602%\n",
      "----\n",
      "Probability of ' user': 65.0680%\n",
      "Probability of ' question': 34.8284%\n",
      "----\n",
      "Probability of ' is': 83.4444%\n",
      "Probability of ' asked': 16.4312%\n",
      "----\n",
      "Probability of ' asking': 99.9996%\n",
      "----\n",
      "Probability of ',': 51.8158%\n",
      "Probability of ' \"': 35.6125%\n",
      "Probability of ' for': 0.9490%\n",
      "Probability of ' what': 11.5617%\n",
      "----\n",
      "Probability of ' \"': 99.9934%\n",
      "----\n",
      "Probability of 'What': 99.9971%\n",
      "----\n",
      "Probability of ' is': 99.9997%\n",
      "----\n",
      "Probability of ' ': 99.9998%\n",
      "----\n",
      "Probability of '1': 99.9992%\n",
      "----\n",
      "Probability of '+': 99.8371%\n",
      "----\n",
      "Probability of '1': 99.9990%\n",
      "----\n",
      "Probability of '?': 9.5342%\n",
      "Probability of '?\"': 90.4577%\n",
      "----\n",
      "Probability of ' and': 92.9506%\n",
      "Probability of ' And': 1.5024%\n",
      "Probability of ' They': 2.1860%\n",
      "Probability of ' Let': 1.5024%\n",
      "Probability of ' Hmm': 1.1701%\n",
      "Probability of ' Alright': 0.4878%\n",
      "----\n",
      "Probability of ' they': 29.4155%\n",
      "Probability of ' wants': 70.5641%\n",
      "----\n",
      "Probability of ' a': 99.9998%\n",
      "----\n",
      "Probability of ' short': 99.9788%\n",
      "----\n",
      "Probability of ' answer': 99.9990%\n",
      "----\n",
      "Probability of '.': 99.9413%\n",
      "----\n",
      "Probability of ' Let': 98.1525%\n",
      "Probability of ' Hmm': 1.4001%\n",
      "Probability of ' Alright': 0.4011%\n",
      "----\n",
      "Probability of ''s': 0.7577%\n",
      "Probability of ' me': 99.2421%\n",
      "----\n",
      "Probability of ' start': 6.4124%\n",
      "Probability of ' break': 4.9940%\n",
      "Probability of ' think': 88.5203%\n",
      "----\n",
      "Probability of '.': 43.7772%\n",
      "Probability of '.\n",
      "\n",
      "': 11.0686%\n",
      "Probability of ' about': 43.7772%\n",
      "Probability of '...': 0.8018%\n",
      "Probability of ' through': 0.4292%\n",
      "Probability of ' here': 0.1230%\n",
      "----\n",
      "Probability of ' ': 0.4839%\n",
      "Probability of ' I': 1.9140%\n",
      "Probability of ' This': 3.1557%\n",
      "Probability of ' At': 1.6891%\n",
      "Probability of ' First': 16.0261%\n",
      "Probability of ' Well': 49.3638%\n",
      "Probability of ' Hmm': 26.4226%\n",
      "Probability of ' Alright': 0.3769%\n",
      "----\n",
      "Probability of ',': 99.9999%\n",
      "----\n",
      "Probability of ' ': 77.4112%\n",
      "Probability of ' the': 0.4603%\n",
      "Probability of ' in': 0.1319%\n",
      "Probability of ' I': 3.4012%\n",
      "Probability of ' this': 5.6076%\n",
      "Probability of ' that': 1.8205%\n",
      "Probability of ' from': 0.4062%\n",
      "Probability of ' at': 4.3672%\n",
      "Probability of ' first': 0.3585%\n",
      "Probability of ' addition': 0.2792%\n",
      "Probability of ' basic': 0.5216%\n",
      "Probability of ' math': 4.9487%\n",
      "----\n",
      "Probability of '1': 99.9995%\n",
      "----\n",
      "Probability of '+': 0.8567%\n",
      "Probability of ' plus': 99.0263%\n",
      "----\n",
      "Probability of ' ': 99.9983%\n",
      "----\n",
      "Probability of '1': 99.9998%\n",
      "----\n",
      "Probability of '.': 0.5872%\n",
      "Probability of ' is': 98.7466%\n",
      "Probability of '...': 0.2448%\n",
      "Probability of ' seems': 0.1020%\n",
      "Probability of ' equals': 0.1485%\n",
      "----\n",
      "Probability of ' ': 0.2424%\n",
      "Probability of ' a': 35.9814%\n",
      "Probability of '...': 0.1470%\n",
      "Probability of ' pretty': 59.3233%\n",
      "Probability of ' basic': 0.4529%\n",
      "Probability of ' definitely': 0.5132%\n",
      "Probability of ' basically': 0.1470%\n",
      "Probability of ' obviously': 0.2747%\n",
      "Probability of ' straightforward': 2.6065%\n",
      "----\n",
      "Probability of ' basic': 46.6617%\n",
      "Probability of ' fundamental': 0.4575%\n",
      "Probability of ' straightforward': 52.8747%\n",
      "----\n",
      "Probability of ',': 14.1013%\n",
      "Probability of '.': 81.1476%\n",
      "Probability of ' in': 1.4863%\n",
      "Probability of '.\n",
      "\n",
      "': 0.2011%\n",
      "Probability of ' addition': 0.2279%\n",
      "Probability of ' basic': 0.4825%\n",
      "Probability of ' math': 2.1625%\n",
      "Probability of ' arithmetic': 0.1220%\n",
      "----\n",
      "Probability of ' I': 16.0113%\n",
      "Probability of ' The': 6.6745%\n",
      "Probability of ' In': 43.5232%\n",
      "Probability of ' It': 6.6745%\n",
      "Probability of ' If': 0.7035%\n",
      "Probability of ' But': 16.0113%\n",
      "Probability of ' At': 1.3143%\n",
      "Probability of ' When': 0.4835%\n",
      "Probability of ' From': 3.1528%\n",
      "Probability of ' Let': 2.7823%\n",
      "Probability of ' Wait': 0.2588%\n",
      "Probability of ' Basic': 0.6208%\n",
      "Probability of ' Adding': 0.7972%\n",
      "Probability of ' Normally': 0.1385%\n",
      "Probability of ' Hmm': 0.2016%\n",
      "----\n",
      "Probability of ' most': 0.2434%\n",
      "Probability of ' basic': 98.1867%\n",
      "Probability of ' arithmetic': 1.4006%\n",
      "Probability of ' mathematics': 0.1150%\n",
      "----\n",
      "Probability of ' addition': 0.4602%\n",
      "Probability of ' math': 0.1164%\n",
      "Probability of ' arithmetic': 99.3748%\n",
      "----\n",
      "Probability of ',': 99.9268%\n",
      "----\n",
      "Probability of ' ': 0.7648%\n",
      "Probability of ' the': 0.1035%\n",
      "Probability of ' that': 0.9820%\n",
      "Probability of ' when': 9.3169%\n",
      "Probability of ' addition': 0.4094%\n",
      "Probability of ' adding': 88.3966%\n",
      "----\n",
      "Probability of ' ': 49.8765%\n",
      "Probability of ' the': 0.1236%\n",
      "Probability of ' one': 49.8765%\n",
      "----\n",
      "Probability of '1': 99.9999%\n",
      "----\n",
      "Probability of ' to': 5.3400%\n",
      "Probability of ' and': 94.6545%\n",
      "----\n",
      "Probability of ' ': 98.5849%\n",
      "Probability of ' another': 1.4062%\n",
      "----\n",
      "Probability of '1': 99.9939%\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "distributions = get_token_distributions(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06763df0",
   "metadata": {},
   "source": [
    "### Entropy + Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d089b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_probabilities(token_prob_dict, path, filename):    \n",
    "    sorted_tokens = token_prob_dict.keys()\n",
    "    sorted_tokens = [\"_\" if token==\" \" else token.replace(\" \", \"_\") for token in sorted_tokens]\n",
    "    probabilities = [p.item() for p in token_prob_dict.values()]\n",
    "    \n",
    "    entropy = -sum(p * np.log(p) for p in probabilities if p > 0) #show entropy too\n",
    "    normalized_entropy = entropy / np.log(len(probabilities))\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.bar(sorted_tokens, probabilities, color='lightblue', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('token', fontsize=14, family='sans-serif')\n",
    "    plt.ylabel('P(token)', fontsize=14, family='sans-serif')\n",
    "    plt.title(f'Token Probabilities\\nNormalized Entropy: {normalized_entropy:.3f}', fontsize=16, family='sans-serif', fontweight='bold')\n",
    "\n",
    "    # Customize ticks and axes\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Clean up spines\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "    # Tight layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    full_path = os.path.join(\"plots\", filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64eb64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/token_0\n",
      "Plot saved to plots/token_1\n",
      "Plot saved to plots/token_2\n",
      "Plot saved to plots/token_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_316/160312386.py:7: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  normalized_entropy = entropy / np.log(len(probabilities))\n",
      "/tmp/ipykernel_316/160312386.py:7: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  normalized_entropy = entropy / np.log(len(probabilities))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/token_4\n",
      "Plot saved to plots/token_5\n",
      "Plot saved to plots/token_6\n",
      "Plot saved to plots/token_7\n",
      "Plot saved to plots/token_8\n",
      "Plot saved to plots/token_9\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "stop = 10\n",
    "for i,token_distribution in enumerate(distributions):\n",
    "    plot_token_probabilities(token_distribution, \"plots\", f\"token_{i}\")\n",
    "    c += 1\n",
    "    if c == stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b6dee4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropies(distributions):\n",
    "    def calculate_entropy(probs, n):\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1).item() \n",
    "        normalized_entropy = entropy / np.log(n) if np.log(n) > 0 else 0\n",
    "        return normalized_entropy\n",
    "    entropies = []\n",
    "    for distribution in distributions:\n",
    "        probabilities = torch.stack(list(distribution.values()))\n",
    "        print(probabilities)\n",
    "        print()\n",
    "        entropies.append(calculate_entropy(probabilities, len(distribution)))\n",
    "    print(f\"Entropy for eachs token distribution: {entropies}\")\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f2326",
   "metadata": {},
   "outputs": [],
   "source": [
    "entropies = get_entropies(distributions)\n",
    "print(\"--check--\")\n",
    "print(entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f2091",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
