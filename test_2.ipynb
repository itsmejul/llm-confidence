{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50d4cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cdbc195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in as M00nl8tshad0w\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder, whoami\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "HfFolder.save_token(hf_token)\n",
    "user = whoami()\n",
    "print(f\"logged in as {user[\"name\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456cfbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory: /home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_models/llama3_70b/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4ef533dd1a4a4c8e53daf960022235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_name, local_dir=\"./models/llama3_70b\"):\n",
    "    if os.path.exists(local_dir):\n",
    "        print(f\"Loading model from local directory: {local_dir}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "        model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "    else:\n",
    "        print(f\"Local directory not found. Downloading model '{model_name}' from Hugging Face Hub...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        tokenizer.save_pretrained(local_dir)\n",
    "        model.save_pretrained(local_dir)\n",
    "        print(f\"Model downloaded and saved locally to: {local_dir}\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(model_name=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "                              local_dir=\"/home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_models/llama3_70b/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33add88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48285e7fad6747c8b254e14eb1c920f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_hf_dataset(dataset_name, subset=\"default\", local_dir=\"~/hf_datasets/OpenR1_Math_220k/\"):\n",
    "    local_dir = os.path.expanduser(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    return load_dataset(dataset_name, subset, cache_dir=local_dir)\n",
    "\n",
    "math_dataset = load_hf_dataset(dataset_name=\"open-r1/OpenR1-Math-220k\",\n",
    "                               local_dir=\"/home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_datasets/open-r1/OpenR1-Math-220k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee876ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d51abc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: GenerateDecoderOnlyOutput(sequences=tensor([[128000,  15724,    374,    279,   1566]], device='cuda:0'), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')), logits=None, attentions=None, hidden_states=None, past_key_values=<transformers.cache_utils.DynamicCache object at 0x764d986a4d70>)\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Print the scores for each token generated with Greedy Search\n",
    "outputs = model.generate(**inputs, max_new_tokens=2, return_dict_in_generate=True, output_scores=True)\n",
    "print(f\"Generated: {outputs}\")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da66ff90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| token | token string | log probability | probability\n",
      "|   279 |  the     | -0.428 | 65.20%\n",
      "|  1566 |  last    | -1.356 | 25.77%\n"
     ]
    }
   ],
   "source": [
    "# input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for encoder-decoder models, like BART or T5.\n",
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "print(\"| token | token string | log probability | probability\")\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | log probability | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().cpu().numpy():.3f} | {np.exp(score.detach().cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.scores  # This is a list of logits for each token generated\n",
    "# Convert logits to probabilities using softmax\n",
    "probabilities = [torch.nn.functional.softmax(logit, dim=-1) for logit in logits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "307a0e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'))\n",
      "torch.Size([1, 128256])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(logits)\n",
    "print(logits[0].shape)\n",
    "print(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fb0b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128256])\n",
      "torch.Size([128256])\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "exp = probabilities[0]\n",
    "print(exp.shape)\n",
    "exp = exp.squeeze(0)\n",
    "print(exp.shape)\n",
    "print(exp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b44782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True,  ..., True, True, True], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prob_values = torch.isclose(exp, torch.tensor(float(0)))\n",
    "print(prob_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ed76c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of ' a': 23.01%\n",
      "Probability of ' the': 65.20%\n",
      "Probability of ' my': 4.35%\n",
      "Probability of ' World': 5.35%\n",
      "Probability of ' National': 2.10%\n",
      "tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "token_probabilities = {}\n",
    "sum_probs = 0\n",
    "for i, bool in enumerate(prob_values):\n",
    "    if not bool:\n",
    "        actual_token = tokenizer.decode(i)\n",
    "        token_prob = exp[i]\n",
    "        print(f\"Probability of '{actual_token}':\", end=' ')\n",
    "        print(f\"{token_prob.item():.2%}\")\n",
    "        token_probabilities[actual_token] = token_prob\n",
    "        sum_probs += token_prob\n",
    "print(sum_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c56f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(probs):\n",
    "    return -torch.sum(probs * torch.log(probs + 1e-10), dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8b3e874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropies for each token: [0.9908984899520874, 1.4603729248046875]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the probabilities of each generated token and compute entropy\n",
    "entropies = []\n",
    "for prob in probabilities:\n",
    "    entropy = calculate_entropy(prob)\n",
    "    entropies.append(entropy)\n",
    "\n",
    "print(\"Entropies for each token:\", entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "beac8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  , Entropy: 0.991\n",
      "Token: t, Entropy: 1.460\n"
     ]
    }
   ],
   "source": [
    "generated_token_ids = outputs.sequences[0][inputs.input_ids.shape[1]:]\n",
    "\n",
    "# Decode token IDs into strings\n",
    "generated_tokens = tokenizer.decode(generated_token_ids, skip_special_tokens=True)\n",
    "\n",
    "for i, (token, entropy) in enumerate(zip(generated_tokens, entropies)):\n",
    "    print(f\"Token: {token}, Entropy: {entropy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9b9f9cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_probabilities(token_prob_dict):\n",
    "    \n",
    "    sorted_tokens = sorted(token_prob_dict.keys())\n",
    "    probabilities = [p.item() for p in token_prob_dict.values()]\n",
    "    #sorted_tokens = sorted(token_prob_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    #tokens, probabilities = zip(*sorted_tokens)\n",
    "\n",
    "    # If probabilities are tensors, detach and move them to numpy\n",
    "    #probabilities = [p.detach().cpu().numpy() for p in probabilities]\n",
    "\n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.bar(sorted_tokens, probabilities, color='blue', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('Tokens', fontsize=14, family='sans-serif')\n",
    "    plt.ylabel('P(token)', fontsize=14, family='sans-serif')\n",
    "    plt.title('Token Probabilities', fontsize=16, family='sans-serif', fontweight='bold')\n",
    "\n",
    "    # Customize ticks and axes\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Clean up spines\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "    # Tight layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aec042",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_probabilities(token_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c96ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
