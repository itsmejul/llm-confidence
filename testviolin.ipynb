{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910ecb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_tensor_path='experiments/llama2_test_cot/output_2025-06-11_11-50.pt'\n",
      "No duplicate questions found.\n",
      "Saved LogTokU quadrant plot to: experiments/llama2_test_cot/logtoku_quadrants.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julian/dev/math-ml/evaluation_utils.py:333: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "experiment_name = \"llama2_test_cot\"\n",
    "rerun = \"no\"\n",
    "\n",
    "experiment_path = os.path.join('experiments', experiment_name)\n",
    "\n",
    "#==========\n",
    "# Metadata\n",
    "#==========\n",
    "with open(f\"{experiment_path}/metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "model_name = metadata[\"model\"]\n",
    "dataset = metadata[\"dataset\"]\n",
    "prompting_technique = metadata['prompting_technique']\n",
    "\n",
    "#==========\n",
    "# Result Tensor\n",
    "#==========\n",
    "if rerun == \"yes\":\n",
    "     reruns = []\n",
    "     for filename in os.listdir(experiment_path):\n",
    "          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\n",
    "               output_tensor_path = os.path.join(experiment_path, filename)\n",
    "          if filename.startswith(\"rerun\") and filename.endswith(\".pt\"):\n",
    "               reruns.append(os.path.join(experiment_path, filename))\n",
    "     \n",
    "     # Load original output tensor\n",
    "     results = torch.load(output_tensor_path)\n",
    "\n",
    "     # Load and merge rerun results\n",
    "     for rerun_path in reruns:\n",
    "          rerun_tensor = torch.load(rerun_path)\n",
    "          results.update(rerun_tensor)  # overwrite buggy samples with rerun results\n",
    "     print(f\"{output_tensor_path=}\")\n",
    "     print(f\"Rerun_paths = {reruns}\")\n",
    "else:\n",
    "     for filename in os.listdir(experiment_path):\n",
    "          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\n",
    "               output_tensor_path = os.path.join(experiment_path, filename)\n",
    "     results = torch.load(output_tensor_path)\n",
    "     print(f\"{output_tensor_path=}\")\n",
    "\n",
    "#==========\n",
    "# Checking for duplicates\n",
    "#==========\n",
    "from evaluation_utils import check_for_duplicate_questions\n",
    "duplicate_entries = check_for_duplicate_questions(exp_tensor=results)\n",
    "if duplicate_entries:\n",
    "    print(\"\\nDUPLICATE QUESTIONS DETECTED:\")\n",
    "    for question, key1, key2 in duplicate_entries:\n",
    "        print(f\"Question: {question}\\nFound in: {key1} and {key2}\\n\")\n",
    "else:\n",
    "    print(\"No duplicate questions found.\")\n",
    "\n",
    "\n",
    "#==========\n",
    "# Evaluation\n",
    "#==========\n",
    "from evaluation_utils import calculate_accuracy, compute_entropy, get_latency, get_tokens_per_prompt, compute_logtoku_uncertainty, plot_logtoku_quadrants, plot_cosine_violin, plot_entropy_violin\n",
    "\n",
    "accuracy, correctness_dict, answer_dict = calculate_accuracy(exp_tensor=results, prompting_technique=prompting_technique)\n",
    "entropy = compute_entropy(exp_tensor=results, prompting_technique=prompting_technique, normalize=True)\n",
    "latency_per_prompt = get_latency(exp_tensor=results)\n",
    "tokens_per_prompt = get_tokens_per_prompt(exp_tensor=results)\n",
    "logtoku_results = compute_logtoku_uncertainty(exp_tensor=results,prompting_technique=prompting_technique)\n",
    "\n",
    "df_answers = pd.DataFrame([(k, v[0], v[1]) for k, v in answer_dict.items()],columns=[\"prompt_id\", \"llm_answer\", \"ground_truth\"])\n",
    "df_correct = pd.DataFrame(list(correctness_dict.items()), columns=[\"prompt_id\", \"correct\"])\n",
    "df_entropy = pd.DataFrame(list(entropy.items()), columns=[\"prompt_id\", \"entropy\"])\n",
    "df_latency = pd.DataFrame(list(latency_per_prompt.items()), columns=[\"prompt_id\", \"latency\"])\n",
    "df_tokens = pd.DataFrame(list(tokens_per_prompt.items()), columns=[\"prompt_id\", \"tokens_used\"])\n",
    "df_logtoku = pd.DataFrame.from_dict(logtoku_results, orient='index').reset_index().rename(columns={'index': 'prompt_id'})\n",
    "\n",
    "# Merge all into a single dataframe on 'prompt_id'\n",
    "df_merged = df_entropy.merge(df_latency, on=\"prompt_id\") \\\n",
    "                      .merge(df_tokens, on=\"prompt_id\") \\\n",
    "                      .merge(df_correct, on=\"prompt_id\") \\\n",
    "                      .merge(df_answers, on=\"prompt_id\") \\\n",
    "                      .merge(df_logtoku, on=\"prompt_id\")\n",
    "df_merged.to_csv(f\"{experiment_path}/evaluation_results.csv\", index=False)\n",
    "\n",
    "#plot logtoku quadrants\n",
    "plot_path = f\"{experiment_path}/logtoku_quadrants.png\"\n",
    "plot_logtoku_quadrants(df_merged, output_path=plot_path)\n",
    "print(f\"Saved LogTokU quadrant plot to: {plot_path}\")\n",
    "\n",
    "#output a list of buggy samples to rerun them later\n",
    "buggy_samples_indices = []\n",
    "for key, value in correctness_dict.items():\n",
    "     if value == \"buggy\":\n",
    "          indice = key.replace(\"prompt\", \"\")\n",
    "          buggy_samples_indices.append(indice)\n",
    "df_buggy_indices = pd.DataFrame(buggy_samples_indices, columns=[\"buggy_prompt_ids\"])\n",
    "df_buggy_indices.to_csv(f\"{experiment_path}/buggy_prompts_to_rerun.csv\")\n",
    "\n",
    "#==========\n",
    "# Compute average values\n",
    "#==========\n",
    "\n",
    "# ===== Entropy over all samples except buggy ones =====\n",
    "try:\n",
    "    entropies_list = list(entropy.values())\n",
    "    cleaned_list = [x for x in entropies_list if x is not None]\n",
    "    average_entropy = sum(cleaned_list) / len(cleaned_list)\n",
    "except ZeroDivisionError:\n",
    "     average_entropy = \"Bug occured.\"\n",
    "\n",
    "# =====Entropy over all correct answered prompts =====\n",
    "df_correct = df_merged[df_merged[\"correct\"] == \"yes\"]\n",
    "if len(df_correct) > 0:\n",
    "     average_entropy_correct = df_correct[\"entropy\"].mean()\n",
    "else:\n",
    "     average_entropy_correct = \"no correct samples\"\n",
    "\n",
    "     \n",
    "\n",
    "#===== Entropy over all incorrect answered prompts =====\n",
    "df_incorrect = df_merged[df_merged[\"correct\"] == \"no\"]\n",
    "if len(df_incorrect) > 0:\n",
    "     average_entropy_incorrect = df_incorrect[\"entropy\"].mean()\n",
    "else:\n",
    "     average_entropy_incorrect = \"no correct samples\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_entropy_violin(df_correct, df_incorrect)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
