{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e16d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "879\n",
      "0.9378845214843752\n",
      "0.8722063331075143\n",
      "0.02864633591998444\n",
      "0.07486132377089082\n"
     ]
    }
   ],
   "source": [
    "# Test how big tensors are and where we can save space\n",
    "import pandas as pd\n",
    "res_path = \"experiments/llama2_cot_all/evaluation_results.csv\"\n",
    "with open(res_path, \"r\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "# Ensure entropy is float\n",
    "df['entropy'] = pd.to_numeric(df['entropy'], errors='coerce')\n",
    "df['cosine'] = pd.to_numeric(df['cosine'], errors='coerce')\n",
    "\n",
    "#df = df[~((df['entropy'] == 0.0) & (df['cosine'] == 1.0))]\n",
    "\n",
    "\n",
    "# Split based on correctness\n",
    "df_correct = df[df['correct'] == 'yes']\n",
    "df_incorrect = df[df['correct'] == 'no']\n",
    "\n",
    "# Then call your method\n",
    "from evaluation_utils import plot_entropy_violin, plot_cosine_violin\n",
    "plot_entropy_violin(df_correct, df_incorrect)\n",
    "plot_cosine_violin(df_correct, df_incorrect)\n",
    "\n",
    "print(df_correct['cosine'].mean())\n",
    "print(df_incorrect['cosine'].mean())\n",
    "print(df_correct['entropy'].mean())\n",
    "print(df_incorrect['entropy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910ecb6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_tensor_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m           \u001b[38;5;28;01mif\u001b[39;00m filename.startswith(\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m filename.endswith(\u001b[33m\"\u001b[39m\u001b[33m.pt\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     46\u001b[39m                output_tensor_path = os.path.join(experiment_path, filename)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m      results = torch.load(\u001b[43moutput_tensor_path\u001b[49m)\n\u001b[32m     48\u001b[39m      \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_tensor_path\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m#==========\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Checking for duplicates\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#==========\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'output_tensor_path' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "experiment_name = \"llama2_cot_all\"\n",
    "rerun = \"no\"\n",
    "\n",
    "experiment_path = os.path.join('experiments', experiment_name)\n",
    "\n",
    "#==========\n",
    "# Metadata\n",
    "#==========\n",
    "with open(f\"{experiment_path}/metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "model_name = metadata[\"model\"]\n",
    "dataset = metadata[\"dataset\"]\n",
    "prompting_technique = metadata['prompting_technique']\n",
    "\n",
    "#==========\n",
    "# Result Tensor\n",
    "#==========\n",
    "if rerun == \"yes\":\n",
    "     reruns = []\n",
    "     for filename in os.listdir(experiment_path):\n",
    "          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\n",
    "               output_tensor_path = os.path.join(experiment_path, filename)\n",
    "          if filename.startswith(\"rerun\") and filename.endswith(\".pt\"):\n",
    "               reruns.append(os.path.join(experiment_path, filename))\n",
    "     \n",
    "     # Load original output tensor\n",
    "     results = torch.load(output_tensor_path)\n",
    "\n",
    "     # Load and merge rerun results\n",
    "     for rerun_path in reruns:\n",
    "          rerun_tensor = torch.load(rerun_path)\n",
    "          results.update(rerun_tensor)  # overwrite buggy samples with rerun results\n",
    "     print(f\"{output_tensor_path=}\")\n",
    "     print(f\"Rerun_paths = {reruns}\")\n",
    "else:\n",
    "     for filename in os.listdir(experiment_path):\n",
    "          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\n",
    "               output_tensor_path = os.path.join(experiment_path, filename)\n",
    "     results = torch.load(output_tensor_path)\n",
    "     print(f\"{output_tensor_path=}\")\n",
    "\n",
    "#==========\n",
    "# Checking for duplicates\n",
    "#==========\n",
    "from evaluation_utils import check_for_duplicate_questions\n",
    "duplicate_entries = check_for_duplicate_questions(exp_tensor=results)\n",
    "if duplicate_entries:\n",
    "    print(\"\\nDUPLICATE QUESTIONS DETECTED:\")\n",
    "    for question, key1, key2 in duplicate_entries:\n",
    "        print(f\"Question: {question}\\nFound in: {key1} and {key2}\\n\")\n",
    "else:\n",
    "    print(\"No duplicate questions found.\")\n",
    "\n",
    "\n",
    "#==========\n",
    "# Evaluation\n",
    "#==========\n",
    "from evaluation_utils import calculate_accuracy, compute_entropy, get_latency, get_tokens_per_prompt, compute_logtoku_uncertainty, plot_logtoku_quadrants, plot_cosine_violin, plot_entropy_violin\n",
    "\n",
    "accuracy, correctness_dict, answer_dict = calculate_accuracy(exp_tensor=results, prompting_technique=prompting_technique)\n",
    "entropy = compute_entropy(exp_tensor=results, prompting_technique=prompting_technique, normalize=True)\n",
    "latency_per_prompt = get_latency(exp_tensor=results)\n",
    "tokens_per_prompt = get_tokens_per_prompt(exp_tensor=results)\n",
    "logtoku_results = compute_logtoku_uncertainty(exp_tensor=results,prompting_technique=prompting_technique)\n",
    "\n",
    "df_answers = pd.DataFrame([(k, v[0], v[1]) for k, v in answer_dict.items()],columns=[\"prompt_id\", \"llm_answer\", \"ground_truth\"])\n",
    "df_correct = pd.DataFrame(list(correctness_dict.items()), columns=[\"prompt_id\", \"correct\"])\n",
    "df_entropy = pd.DataFrame(list(entropy.items()), columns=[\"prompt_id\", \"entropy\"])\n",
    "df_latency = pd.DataFrame(list(latency_per_prompt.items()), columns=[\"prompt_id\", \"latency\"])\n",
    "df_tokens = pd.DataFrame(list(tokens_per_prompt.items()), columns=[\"prompt_id\", \"tokens_used\"])\n",
    "df_logtoku = pd.DataFrame.from_dict(logtoku_results, orient='index').reset_index().rename(columns={'index': 'prompt_id'})\n",
    "\n",
    "# Merge all into a single dataframe on 'prompt_id'\n",
    "df_merged = df_entropy.merge(df_latency, on=\"prompt_id\") \\\n",
    "                      .merge(df_tokens, on=\"prompt_id\") \\\n",
    "                      .merge(df_correct, on=\"prompt_id\") \\\n",
    "                      .merge(df_answers, on=\"prompt_id\") \\\n",
    "                      .merge(df_logtoku, on=\"prompt_id\")\n",
    "df_merged.to_csv(f\"{experiment_path}/evaluation_results.csv\", index=False)\n",
    "\n",
    "#plot logtoku quadrants\n",
    "plot_path = f\"{experiment_path}/logtoku_quadrants.png\"\n",
    "plot_logtoku_quadrants(df_merged, output_path=plot_path)\n",
    "print(f\"Saved LogTokU quadrant plot to: {plot_path}\")\n",
    "\n",
    "#output a list of buggy samples to rerun them later\n",
    "buggy_samples_indices = []\n",
    "for key, value in correctness_dict.items():\n",
    "     if value == \"buggy\":\n",
    "          indice = key.replace(\"prompt\", \"\")\n",
    "          buggy_samples_indices.append(indice)\n",
    "df_buggy_indices = pd.DataFrame(buggy_samples_indices, columns=[\"buggy_prompt_ids\"])\n",
    "df_buggy_indices.to_csv(f\"{experiment_path}/buggy_prompts_to_rerun.csv\")\n",
    "\n",
    "#==========\n",
    "# Compute average values\n",
    "#==========\n",
    "\n",
    "# ===== Entropy over all samples except buggy ones =====\n",
    "try:\n",
    "    entropies_list = list(entropy.values())\n",
    "    cleaned_list = [x for x in entropies_list if x is not None]\n",
    "    average_entropy = sum(cleaned_list) / len(cleaned_list)\n",
    "except ZeroDivisionError:\n",
    "     average_entropy = \"Bug occured.\"\n",
    "\n",
    "# =====Entropy over all correct answered prompts =====\n",
    "df_correct = df_merged[df_merged[\"correct\"] == \"yes\"]\n",
    "if len(df_correct) > 0:\n",
    "     average_entropy_correct = df_correct[\"entropy\"].mean()\n",
    "else:\n",
    "     average_entropy_correct = \"no correct samples\"\n",
    "\n",
    "     \n",
    "\n",
    "#===== Entropy over all incorrect answered prompts =====\n",
    "df_incorrect = df_merged[df_merged[\"correct\"] == \"no\"]\n",
    "if len(df_incorrect) > 0:\n",
    "     average_entropy_incorrect = df_incorrect[\"entropy\"].mean()\n",
    "else:\n",
    "     average_entropy_incorrect = \"no correct samples\"\n",
    "\n",
    "\n",
    "print(len(df_incorrect))\n",
    "\n",
    "plot_entropy_violin(df_correct, df_incorrect)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
