{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3e16d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1169\n",
      "0.9295257404286374\n",
      "0.870512764160731\n",
      "0.03618655971986007\n",
      "0.07651943024570201\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # add project root to path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))  # or use pathlib\n",
    "# Test how big tensors are and where we can save space\n",
    "import pandas as pd\n",
    "experiment_name = \"llama2_cot_all\"\n",
    "experiment_path = \"../results/accuracy_exp/\" + experiment_name\n",
    "\n",
    "res_path = experiment_path + \"/evaluation_results.csv\"\n",
    "with open(res_path, \"r\") as f:\n",
    "    df = pd.read_csv(f)\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "# Ensure entropy is float\n",
    "df['entropy'] = pd.to_numeric(df['entropy'], errors='coerce')\n",
    "df['cosine'] = pd.to_numeric(df['cosine'], errors='coerce')\n",
    "\n",
    "#df = df[~((df['entropy'] == 0.0) & (df['cosine'] == 1.0))]\n",
    "\n",
    "\n",
    "# Split based on correctness\n",
    "df_correct = df[df['correct'] == 'yes']\n",
    "df_incorrect = df[df['correct'] == 'no']\n",
    "\n",
    "# Then call your method\n",
    "from accuracy.evaluation_utils import plot_entropy_violin, plot_cosine_violin\n",
    "plot_entropy_violin(df_correct, df_incorrect, experiment_path)\n",
    "plot_cosine_violin(df_correct, df_incorrect, experiment_path)\n",
    "\n",
    "print(df_correct['cosine'].mean())\n",
    "print(df_incorrect['cosine'].mean())\n",
    "print(df_correct['entropy'].mean())\n",
    "print(df_incorrect['entropy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "910ecb6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport torch\\nimport json\\nimport argparse\\nimport os\\nimport pandas as pd\\n\\n\\nexperiment_name = \"qwen_baseline_50\"\\nrerun = \"no\"\\n\\nexperiment_path = os.path.join(\\'results\\',\\'accuracy_exp\\', experiment_name)\\n\\n#==========\\n# Metadata\\n#==========\\nwith open(f\"{experiment_path}/metadata.json\", \"r\") as f:\\n    metadata = json.load(f)\\n\\nmodel_name = metadata[\"model\"]\\ndataset = metadata[\"dataset\"]\\nprompting_technique = metadata[\\'prompting_technique\\']\\n\\n#==========\\n# Result Tensor\\n#==========\\nif rerun == \"yes\":\\n     reruns = []\\n     for filename in os.listdir(experiment_path):\\n          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\\n               output_tensor_path = os.path.join(experiment_path, filename)\\n          if filename.startswith(\"rerun\") and filename.endswith(\".pt\"):\\n               reruns.append(os.path.join(experiment_path, filename))\\n\\n     # Load original output tensor\\n     results = torch.load(output_tensor_path)\\n\\n     # Load and merge rerun results\\n     for rerun_path in reruns:\\n          rerun_tensor = torch.load(rerun_path)\\n          results.update(rerun_tensor)  # overwrite buggy samples with rerun results\\n     print(f\"{output_tensor_path=}\")\\n     print(f\"Rerun_paths = {reruns}\")\\nelse:\\n     for filename in os.listdir(experiment_path):\\n          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\\n               output_tensor_path = os.path.join(experiment_path, filename)\\n     results = torch.load(output_tensor_path)\\n     print(f\"{output_tensor_path=}\")\\n\\n#==========\\n# Checking for duplicates\\n#==========\\nfrom accuracy.evaluation_utils import check_for_duplicate_questions\\nduplicate_entries = check_for_duplicate_questions(exp_tensor=results)\\nif duplicate_entries:\\n    print(\"\\nDUPLICATE QUESTIONS DETECTED:\")\\n    for question, key1, key2 in duplicate_entries:\\n        print(f\"Question: {question}\\nFound in: {key1} and {key2}\\n\")\\nelse:\\n    print(\"No duplicate questions found.\")\\n\\n\\n#==========\\n# Evaluation\\n#==========\\nfrom accuracy.evaluation_utils import calculate_accuracy, compute_entropy, get_latency, get_tokens_per_prompt, compute_logtoku_uncertainty, plot_logtoku_quadrants, plot_cosine_violin, plot_entropy_violin\\n\\naccuracy, correctness_dict, answer_dict = calculate_accuracy(exp_tensor=results, prompting_technique=prompting_technique)\\nentropy = compute_entropy(exp_tensor=results, prompting_technique=prompting_technique, normalize=True)\\nlatency_per_prompt = get_latency(exp_tensor=results)\\ntokens_per_prompt = get_tokens_per_prompt(exp_tensor=results)\\nlogtoku_results = compute_logtoku_uncertainty(exp_tensor=results,prompting_technique=prompting_technique)\\n\\ndf_answers = pd.DataFrame([(k, v[0], v[1]) for k, v in answer_dict.items()],columns=[\"prompt_id\", \"llm_answer\", \"ground_truth\"])\\ndf_correct = pd.DataFrame(list(correctness_dict.items()), columns=[\"prompt_id\", \"correct\"])\\ndf_entropy = pd.DataFrame(list(entropy.items()), columns=[\"prompt_id\", \"entropy\"])\\ndf_latency = pd.DataFrame(list(latency_per_prompt.items()), columns=[\"prompt_id\", \"latency\"])\\ndf_tokens = pd.DataFrame(list(tokens_per_prompt.items()), columns=[\"prompt_id\", \"tokens_used\"])\\ndf_logtoku = pd.DataFrame.from_dict(logtoku_results, orient=\\'index\\').reset_index().rename(columns={\\'index\\': \\'prompt_id\\'})\\n\\n# Merge all into a single dataframe on \\'prompt_id\\'\\ndf_merged = df_entropy.merge(df_latency, on=\"prompt_id\")                       .merge(df_tokens, on=\"prompt_id\")                       .merge(df_correct, on=\"prompt_id\")                       .merge(df_answers, on=\"prompt_id\")                       .merge(df_logtoku, on=\"prompt_id\")\\ndf_merged.to_csv(f\"{experiment_path}/evaluation_results.csv\", index=False)\\n\\n#plot logtoku quadrants\\nplot_path = f\"{experiment_path}/logtoku_quadrants.png\"\\nplot_logtoku_quadrants(df_merged, output_path=plot_path)\\nprint(f\"Saved LogTokU quadrant plot to: {plot_path}\")\\n\\n#output a list of buggy samples to rerun them later\\nbuggy_samples_indices = []\\nfor key, value in correctness_dict.items():\\n     if value == \"buggy\":\\n          indice = key.replace(\"prompt\", \"\")\\n          buggy_samples_indices.append(indice)\\ndf_buggy_indices = pd.DataFrame(buggy_samples_indices, columns=[\"buggy_prompt_ids\"])\\ndf_buggy_indices.to_csv(f\"{experiment_path}/buggy_prompts_to_rerun.csv\")\\n\\n#==========\\n# Compute average values\\n#==========\\n\\n# ===== Entropy over all samples except buggy ones =====\\ntry:\\n    entropies_list = list(entropy.values())\\n    cleaned_list = [x for x in entropies_list if x is not None]\\n    average_entropy = sum(cleaned_list) / len(cleaned_list)\\nexcept ZeroDivisionError:\\n     average_entropy = \"Bug occured.\"\\n\\n# =====Entropy over all correct answered prompts =====\\ndf_correct = df_merged[df_merged[\"correct\"] == \"yes\"]\\nif len(df_correct) > 0:\\n     average_entropy_correct = df_correct[\"entropy\"].mean()\\nelse:\\n     average_entropy_correct = \"no correct samples\"\\n\\n\\n\\n#===== Entropy over all incorrect answered prompts =====\\ndf_incorrect = df_merged[df_merged[\"correct\"] == \"no\"]\\nif len(df_incorrect) > 0:\\n     average_entropy_incorrect = df_incorrect[\"entropy\"].mean()\\nelse:\\n     average_entropy_incorrect = \"no correct samples\"\\n\\n\\nprint(len(df_incorrect))\\n\\nplot_entropy_violin(df_correct, df_incorrect, experiment_path)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import json\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "experiment_name = \"qwen_baseline_50\"\n",
    "rerun = \"no\"\n",
    "\n",
    "experiment_path = os.path.join('results','accuracy_exp', experiment_name)\n",
    "\n",
    "#==========\n",
    "# Metadata\n",
    "#==========\n",
    "with open(f\"{experiment_path}/metadata.json\", \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "model_name = metadata[\"model\"]\n",
    "dataset = metadata[\"dataset\"]\n",
    "prompting_technique = metadata['prompting_technique']\n",
    "\n",
    "#==========\n",
    "# Result Tensor\n",
    "#==========\n",
    "if rerun == \"yes\":\n",
    "     reruns = []\n",
    "     for filename in os.listdir(experiment_path):\n",
    "          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\n",
    "               output_tensor_path = os.path.join(experiment_path, filename)\n",
    "          if filename.startswith(\"rerun\") and filename.endswith(\".pt\"):\n",
    "               reruns.append(os.path.join(experiment_path, filename))\n",
    "     \n",
    "     # Load original output tensor\n",
    "     results = torch.load(output_tensor_path)\n",
    "\n",
    "     # Load and merge rerun results\n",
    "     for rerun_path in reruns:\n",
    "          rerun_tensor = torch.load(rerun_path)\n",
    "          results.update(rerun_tensor)  # overwrite buggy samples with rerun results\n",
    "     print(f\"{output_tensor_path=}\")\n",
    "     print(f\"Rerun_paths = {reruns}\")\n",
    "else:\n",
    "     for filename in os.listdir(experiment_path):\n",
    "          if filename.startswith(\"output\") and filename.endswith(\".pt\"):\n",
    "               output_tensor_path = os.path.join(experiment_path, filename)\n",
    "     results = torch.load(output_tensor_path)\n",
    "     print(f\"{output_tensor_path=}\")\n",
    "\n",
    "#==========\n",
    "# Checking for duplicates\n",
    "#==========\n",
    "from accuracy.evaluation_utils import check_for_duplicate_questions\n",
    "duplicate_entries = check_for_duplicate_questions(exp_tensor=results)\n",
    "if duplicate_entries:\n",
    "    print(\"\\nDUPLICATE QUESTIONS DETECTED:\")\n",
    "    for question, key1, key2 in duplicate_entries:\n",
    "        print(f\"Question: {question}\\nFound in: {key1} and {key2}\\n\")\n",
    "else:\n",
    "    print(\"No duplicate questions found.\")\n",
    "\n",
    "\n",
    "#==========\n",
    "# Evaluation\n",
    "#==========\n",
    "from accuracy.evaluation_utils import calculate_accuracy, compute_entropy, get_latency, get_tokens_per_prompt, compute_logtoku_uncertainty, plot_logtoku_quadrants, plot_cosine_violin, plot_entropy_violin\n",
    "\n",
    "accuracy, correctness_dict, answer_dict = calculate_accuracy(exp_tensor=results, prompting_technique=prompting_technique)\n",
    "entropy = compute_entropy(exp_tensor=results, prompting_technique=prompting_technique, normalize=True)\n",
    "latency_per_prompt = get_latency(exp_tensor=results)\n",
    "tokens_per_prompt = get_tokens_per_prompt(exp_tensor=results)\n",
    "logtoku_results = compute_logtoku_uncertainty(exp_tensor=results,prompting_technique=prompting_technique)\n",
    "\n",
    "df_answers = pd.DataFrame([(k, v[0], v[1]) for k, v in answer_dict.items()],columns=[\"prompt_id\", \"llm_answer\", \"ground_truth\"])\n",
    "df_correct = pd.DataFrame(list(correctness_dict.items()), columns=[\"prompt_id\", \"correct\"])\n",
    "df_entropy = pd.DataFrame(list(entropy.items()), columns=[\"prompt_id\", \"entropy\"])\n",
    "df_latency = pd.DataFrame(list(latency_per_prompt.items()), columns=[\"prompt_id\", \"latency\"])\n",
    "df_tokens = pd.DataFrame(list(tokens_per_prompt.items()), columns=[\"prompt_id\", \"tokens_used\"])\n",
    "df_logtoku = pd.DataFrame.from_dict(logtoku_results, orient='index').reset_index().rename(columns={'index': 'prompt_id'})\n",
    "\n",
    "# Merge all into a single dataframe on 'prompt_id'\n",
    "df_merged = df_entropy.merge(df_latency, on=\"prompt_id\") \\\n",
    "                      .merge(df_tokens, on=\"prompt_id\") \\\n",
    "                      .merge(df_correct, on=\"prompt_id\") \\\n",
    "                      .merge(df_answers, on=\"prompt_id\") \\\n",
    "                      .merge(df_logtoku, on=\"prompt_id\")\n",
    "df_merged.to_csv(f\"{experiment_path}/evaluation_results.csv\", index=False)\n",
    "\n",
    "#plot logtoku quadrants\n",
    "plot_path = f\"{experiment_path}/logtoku_quadrants.png\"\n",
    "plot_logtoku_quadrants(df_merged, output_path=plot_path)\n",
    "print(f\"Saved LogTokU quadrant plot to: {plot_path}\")\n",
    "\n",
    "#output a list of buggy samples to rerun them later\n",
    "buggy_samples_indices = []\n",
    "for key, value in correctness_dict.items():\n",
    "     if value == \"buggy\":\n",
    "          indice = key.replace(\"prompt\", \"\")\n",
    "          buggy_samples_indices.append(indice)\n",
    "df_buggy_indices = pd.DataFrame(buggy_samples_indices, columns=[\"buggy_prompt_ids\"])\n",
    "df_buggy_indices.to_csv(f\"{experiment_path}/buggy_prompts_to_rerun.csv\")\n",
    "\n",
    "#==========\n",
    "# Compute average values\n",
    "#==========\n",
    "\n",
    "# ===== Entropy over all samples except buggy ones =====\n",
    "try:\n",
    "    entropies_list = list(entropy.values())\n",
    "    cleaned_list = [x for x in entropies_list if x is not None]\n",
    "    average_entropy = sum(cleaned_list) / len(cleaned_list)\n",
    "except ZeroDivisionError:\n",
    "     average_entropy = \"Bug occured.\"\n",
    "\n",
    "# =====Entropy over all correct answered prompts =====\n",
    "df_correct = df_merged[df_merged[\"correct\"] == \"yes\"]\n",
    "if len(df_correct) > 0:\n",
    "     average_entropy_correct = df_correct[\"entropy\"].mean()\n",
    "else:\n",
    "     average_entropy_correct = \"no correct samples\"\n",
    "\n",
    "     \n",
    "\n",
    "#===== Entropy over all incorrect answered prompts =====\n",
    "df_incorrect = df_merged[df_merged[\"correct\"] == \"no\"]\n",
    "if len(df_incorrect) > 0:\n",
    "     average_entropy_incorrect = df_incorrect[\"entropy\"].mean()\n",
    "else:\n",
    "     average_entropy_incorrect = \"no correct samples\"\n",
    "\n",
    "\n",
    "print(len(df_incorrect))\n",
    "\n",
    "plot_entropy_violin(df_correct, df_incorrect, experiment_path)\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
