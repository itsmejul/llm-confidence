{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50c49bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fdaeeb",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914a6c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2a79e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in as M00nl8tshad0w\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfFolder, whoami\n",
    "load_dotenv()\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "HfFolder.save_token(hf_token)\n",
    "user = whoami()\n",
    "print(f\"logged in as {user[\"name\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9b51df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local directory: /home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_models/llama3_70b/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc128b2441494dc9b6014a59cacecb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "def load_model(model_name, local_dir=\"./models/llama3_70b\"):\n",
    "    if os.path.exists(local_dir):\n",
    "        print(f\"Loading model from local directory: {local_dir}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
    "        model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "    else:\n",
    "        print(f\"Local directory not found. Downloading model '{model_name}' from Hugging Face Hub...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        tokenizer.save_pretrained(local_dir)\n",
    "        model.save_pretrained(local_dir)\n",
    "        print(f\"Model downloaded and saved locally to: {local_dir}\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(model_name=\"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "                              local_dir=\"/home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_models/llama3_70b/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7b2be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a13007489a4f44cab7d696cef0c91074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_hf_dataset(dataset_name, subset=\"default\", local_dir=\"~/hf_datasets/OpenR1_Math_220k/\"):\n",
    "    local_dir = os.path.expanduser(local_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    return load_dataset(dataset_name, subset, cache_dir=local_dir)\n",
    "\n",
    "math_dataset = load_hf_dataset(dataset_name=\"open-r1/OpenR1-Math-220k\",\n",
    "                               local_dir=\"/home/max/Studium/Leipzig/Semster6/Math_and_ML/hf_datasets/open-r1/OpenR1-Math-220k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1231523",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0d4f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73427ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: GenerateDecoderOnlyOutput(sequences=tensor([[128000,  15724,    374,    279,   1176]], device='cuda:0'), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0'), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')), logits=None, attentions=None, hidden_states=None, past_key_values=<transformers.cache_utils.DynamicCache object at 0x731944cf75f0>)\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Print the scores for each token generated with Greedy Search\n",
    "outputs = model.generate(**inputs, max_new_tokens=2, return_dict_in_generate=True, output_scores=True)\n",
    "print(f\"Generated: {outputs}\")\n",
    "transition_scores = model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9727bc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| token | token string | log probability | probability\n",
      "|   279 |  the     | -0.428 | 65.20%\n",
      "|  1176 |  first   | -1.773 | 16.99%\n"
     ]
    }
   ],
   "source": [
    "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "generated_tokens = outputs.sequences[:, input_length:]\n",
    "print(\"| token | token string | log probability | probability\")\n",
    "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "    # | token | token string | log probability | probability\n",
    "    print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.detach().cpu().numpy():.3f} | {np.exp(score.detach().cpu().numpy()):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea0f41",
   "metadata": {},
   "source": [
    "### Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdf97769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(logits)=2\n",
      "logits[0].shape=torch.Size([1, 128256])\n",
      "logits[0]=tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]], device='cuda:0')\n",
      "probabilities[0].shape=torch.Size([1, 128256])\n",
      "probabilities[0]=tensor([[0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "logits = outputs.scores  # This is a list of logits for each token generated\n",
    "print(f\"{len(logits)=}\")\n",
    "print(f\"{logits[0].shape=}\")\n",
    "print(f\"{logits[0]=}\")\n",
    "probabilities = [torch.nn.functional.softmax(logit, dim=-1) for logit in logits] # Convert logits to probabilities using softmax\n",
    "print(f\"{probabilities[0].shape=}\")\n",
    "print(f\"{probabilities[0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93e86ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_distributions(probabilites: list) -> list:\n",
    "    distributions = []\n",
    "    for token in probabilites:\n",
    "        token_probabilities = {}\n",
    "        token_tensor = token.squeeze(0)\n",
    "        prob_values = torch.isclose(token_tensor, torch.tensor(float(0)))\n",
    "        for i, bool in enumerate(prob_values):\n",
    "            if not bool:\n",
    "                actual_token = tokenizer.decode(i)\n",
    "                token_prob = token_tensor[i]\n",
    "                print(f\"Probability of '{actual_token}':\", end=' ')\n",
    "                print(f\"{token_prob.item():.2%}\")\n",
    "                token_probabilities[actual_token] = token_prob\n",
    "        print(\"----\")\n",
    "        distributions.append(token_probabilities)\n",
    "    return distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66f4d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of ' a': 23.01%\n",
      "Probability of ' the': 65.20%\n",
      "Probability of ' my': 4.35%\n",
      "Probability of ' World': 5.35%\n",
      "Probability of ' National': 2.10%\n",
      "----\n",
      "Probability of ' ': 25.77%\n",
      "Probability of ' first': 16.99%\n",
      "Probability of ' last': 25.77%\n",
      "Probability of ' day': 28.59%\n",
      "Probability of ' birthday': 2.89%\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "distributions = get_token_distributions(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06763df0",
   "metadata": {},
   "source": [
    "### Entropy + Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d089b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_probabilities(token_prob_dict, path, filename):    \n",
    "    sorted_tokens = token_prob_dict.keys()\n",
    "    sorted_tokens = [\"_\" if token==\" \" else token.replace(\" \", \"_\") for token in sorted_tokens]\n",
    "    probabilities = [p.item() for p in token_prob_dict.values()]\n",
    "    \n",
    "    entropy = -sum(p * np.log(p) for p in probabilities if p > 0) #show entropy too\n",
    "    normalized_entropy = entropy / np.log(len(probabilities))\n",
    "    \n",
    "    # Create a figure and axis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Create the bar plot\n",
    "    plt.bar(sorted_tokens, probabilities, color='lightblue', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('Tokens', fontsize=14, family='sans-serif')\n",
    "    plt.ylabel('P(token)', fontsize=14, family='sans-serif')\n",
    "    plt.title(f'Token Probabilities\\nNormalized Entropy: {normalized_entropy:.3f}', fontsize=16, family='sans-serif', fontweight='bold')\n",
    "\n",
    "    # Customize ticks and axes\n",
    "    plt.xticks(rotation=0, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Clean up spines\n",
    "    ax = plt.gca()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_linewidth(0.5)\n",
    "    ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "    # Tight layout for better spacing\n",
    "    plt.tight_layout()\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    full_path = os.path.join(\"plots\", filename)\n",
    "    plt.savefig(full_path)\n",
    "    plt.close()\n",
    "    print(f\"Plot saved to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64eb64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to plots/token_0\n",
      "Plot saved to plots/token_1\n"
     ]
    }
   ],
   "source": [
    "for i,token_distribution in enumerate(distributions):\n",
    "    plot_token_probabilities(token_distribution, \"plots\", f\"token_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b6dee4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropies(distributions):\n",
    "    def calculate_entropy(probs, n):\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1).item() \n",
    "        normalized_entropy = entropy / np.log(n) if np.log(n) > 0 else 0\n",
    "        return normalized_entropy\n",
    "    entropies = []\n",
    "    for distribution in distributions:\n",
    "        probabilities = torch.stack(list(distribution.values()))\n",
    "        print(probabilities)\n",
    "        entropies.append(calculate_entropy(probabilities, len(distribution)))\n",
    "    print(f\"Entropy for eachs token distribution: {entropies}\")\n",
    "    return entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "670f2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2301, 0.6520, 0.0435, 0.0535, 0.0210], device='cuda:0')\n",
      "tensor([0.2577, 0.1699, 0.2577, 0.2859, 0.0289], device='cuda:0')\n",
      "Entropy for eachs token distribution: [np.float64(0.6156799224784946), np.float64(0.9073807156661494)]\n",
      "--check--\n",
      "[np.float64(0.6156799224784946), np.float64(0.9073807156661494)]\n"
     ]
    }
   ],
   "source": [
    "entropies = get_entropies(distributions)\n",
    "print(\"--check--\")\n",
    "print(entropies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
